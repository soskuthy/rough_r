---
title: "Data processing for rough R: English roughness norms for analysis A1"
author: "Márton Sóskuthy"
output: html_document
---

This script takes the raw data file from Stadtlander & Murdoch (2000) and creates a clean version for analysis.

Loading packages & data.

```{r}
library(tidyverse)
stadt <- read_csv('../raw_data/a1_english_norms/stadtlander_murdoch_2000.csv')
```

Removing extraneous columns / rows, adding descriptive column names

```{r}
# poorly formatted header rows
stadt <- stadt[-c(1:5), ]
# extra info at bottom of the file
stadt <- stadt[-c(308:310), ]
# columns with ratings that are not relevant to our analysis
stadt <- stadt[, -c(2:4, 7:8, 11:ncol(stadt))]
# column names
colnames(stadt) <- c('Word', 'Hard.M', 'Hard.SD', 'Rough.M', 'Rough.SD')
```

Processing based on word labels.

```{r}
# some words have an asterisk following them; remove it
stadt <- mutate(stadt,
	Word = str_replace(Word, '\\*', ''))
# get rid of rows with word labels that have spaces (compounds, phrases etc.)
stadt <- filter(stadt,
	!str_detect(Word, ' '))
# manually removing "wafflish" (not a word!)
stadt <- filter(stadt, 
  Word != "wafflish")
```

Processing based on roughness / hardness values.

```{r}
# remove observations with no roughness rating
stadt <- filter(stadt,
	!is.na(Rough.M))
# make all relevant columns numeric
stadt <- mutate(stadt,
	Rough.M = as.numeric(Rough.M),
	Rough.SD = as.numeric(Rough.SD))
```

Adding pronunciation / part-of-speech data.

```{r}
# read in pronunciation data
ELP <- read_csv('../raw_data/a1_english_norms/ELP_pronunciations.csv')
# join ELP pronunciations into Stadtlander dataset:
stadt <- left_join(stadt, ELP)
# get POS data
SUBTL <- read_csv('../raw_data/a1_english_norms/SUBTLEX_POS.csv')
stadt <- left_join(stadt, SUBTL)
# all POS NA's turn out to be adjectives
stadt[is.na(stadt$POS), ]$POS <- 'Adjective'
# Get rid of non-adjectives (from 123 words to 100):
stadt <- filter(stadt, POS == 'Adjective')
```

Manually adding missing pronunciation data.

```{r}
# Where possible, the root was identified in the ELP pronunciations
# In addition, MacMillan and Webster were consulted for this

stadt[stadt$Word == 'callused',]$Pron <- 'k"a.l@st'
stadt[stadt$Word == 'craterous',]$Pron <- 'kr"e.4@`.r@s'
stadt[stadt$Word == 'goopy',]$Pron <- 'g"u.pi'
stadt[stadt$Word == 'grainy',]$Pron <- 'gr"en.i'
stadt[stadt$Word == 'holey',]$Pron <- 'h"ol.i'
stadt[stadt$Word == 'scrunchy',]$Pron <- 'skr"VntS.i'

# Replace inter-medial dentals with their voiced/voiceless counterparts:

stadt[stadt$Pron == 'b"i4.@d',]$Pron <- 'b"id.@d'
stadt[stadt$Pron == 'k"o4.@d',]$Pron <- 'k"ot.@d'
stadt[stadt$Pron == 'k@`.r"o4.@d',]$Pron <- 'k@`.r"od.@d'
stadt[stadt$Pron == 'd"En4.@d',]$Pron <- 'd"Ent.@d'
stadt[stadt$Pron == 'dIs.dZ"OIn4.@d',]$Pron <- 'dIs.dZ"OInt.@d'
stadt[stadt$Pron == 'dZ@.l"a.4@.n@s',]$Pron <- 'dZ@.l"a.t@.n@s'
stadt[stadt$Pron == 'l"u.br@.k%e4.@d',]$Pron <- 'l"u.br@.k%et.@d'
stadt[stadt$Pron == 'm"a4.@d',]$Pron <- 'm"at.@d'
stadt[stadt$Pron == 'p"en4.@d',]$Pron <- 'p"ent.@d'
stadt[stadt$Pron == 'p"3`.f@`.r%e4.@d',]$Pron <- 'p"3`.f@`.r%et.@d'
stadt[stadt$Pron == 's"an4.i',]$Pron <- 's"and.i'
stadt[stadt$Pron == 'v"El.v@4.i',]$Pron <- 'v"El.v@t.i'
stadt[stadt$Pron == 'sI.r"e4.@d',]$Pron <- 'sI.r"et.@d'
stadt[stadt$Pron == 'kr"e.4@`.r@s',]$Pron <- 'kr"e.t@`.r@s'

# The "`" is either interpreted as /r/ (for rhotic dialects) or as empty for non-rhotics:
# (note that this transcription system interprets intervocalic /r/'s as ambisyllabic, see e.g. /kr"e.t@`.r@s/)

stadt <- mutate(stadt,
	Pron = str_replace_all(Pron, '`', 'r'))

# replace syllabic segments, diphthongs and affricates with unique symbols
# (using celex DISC transcription system where possible, and other R-formula compatible symbols elsewhere)

stadt <- mutate(stadt,
  Pron = str_replace_all(Pron, '@', 'Q'),
	Pron = str_replace_all(Pron, 'n=', 'K'),
	Pron = str_replace_all(Pron, 'l=', 'L'),
	Pron = str_replace_all(Pron, 'aI', '2'),
	Pron = str_replace_all(Pron, 'OI', '4'),
	Pron = str_replace_all(Pron, 'tS', 'J'),
	Pron = str_replace_all(Pron, 'dZ', '_'))
```

We now locate (1) onsets, (2) onsets in stressed syllables, (3) rhymes and (4) rhymes in stressed syllables. We add each segment type in each position as a predictor to our data set (each of which codes "does position X have segment Y?" as a binary predictor), as well as a position-free version of the segments. We start by creating a list of unique segments / consonants / vowels.

```{r}
# find unique segments
unique_segments <- unique(unlist(str_split(stadt$Pron, pattern="")))
# remove stress marks, syllable boundaries, syllabic segments
unique_segments <- unique_segments[!(unique_segments %in% c(".",'\"',"%","K","L"))]
# consonants vs. vowels
consonants <- c("b", "r", "s", "v", "l", "d", "n", "t", "m", "p", "k", "J", "N", "_", "f", "D", "z", "g", "j", "h", "S", "w", "T")
vowels <- unique_segments[!(unique_segments %in% consonants)]
# regex for consonants / vowels
consonants_regex <- paste0("[", paste0(consonants, collapse=""), "]")
consonants_with_syllabic_regex <- paste0("[", paste0(c(consonants, "KL"), collapse=""), "]")
vowels_regex <- paste0("[", paste0(vowels, collapse=""), "]")
```

We now locate these segments in different syllabic positions.

```{r}
# all onset segments
stadt$onset <- stadt$Pron %>%
  str_extract_all(paste0('(^', consonants_regex, '+|[.]', consonants_regex, "+)")) %>%
  lapply(paste0, collapse="") %>%
  unlist()

# stressed onset segments
stadt$stressed_onset <- stadt$Pron %>% 
  str_extract_all(paste0('(^', consonants_regex, '+["%]|[.]', consonants_regex, '+["%])')) %>%
  lapply(paste0, collapse="") %>%
  unlist()

# all rhyme segments
stadt$rhyme <- stadt$Pron %>% 
  str_extract_all(paste0('(', vowels_regex, '*', consonants_with_syllabic_regex, '*[.]|', vowels_regex, '*', consonants_with_syllabic_regex, "*$)")) %>%
  lapply(paste0, collapse="") %>%
  unlist() %>%
  str_replace_all("K", "n") %>%
  str_replace_all("L", "l")

# stressed rhyme segments
stadt$stressed_rhyme <- stadt$Pron %>% 
  str_extract_all(paste0('[%"](', vowels_regex, '*', consonants_with_syllabic_regex, '*[.]|', vowels_regex, '*', consonants_with_syllabic_regex, "*$)")) %>%
  lapply(paste0, collapse="") %>%
  unlist()
```

Setting up indicator variables for each segment for each type of position.

```{r}
# function for checking for segments in specific positions
# argument 1: vector of strings with all segments in relevant positions
# argument 2: list of segments to check for

find_segments <- function (strings, segments) {
  out_matrix <- matrix(0, nrow=length(strings), ncol=length(segments))
  colnames(out_matrix) <- segments
  for (s in 1:length(segments)) {
    segment <- segments[s]
    out_matrix[,s] <- str_detect(strings, segment)
  }
  return(as_tibble(out_matrix))
}

# segments in different positions
# whole word
whole_word_prons <- stadt$Pron %>%
  str_replace_all("K", "n") %>%
  str_replace_all("L", "l")
whole_word_segs <- find_segments(whole_word_prons, unique_segments)
colnames(whole_word_segs) <- paste0("all.", colnames(whole_word_segs))
stadt <- bind_cols(stadt, whole_word_segs)
# onset
onset_segs <- find_segments(stadt$onset, consonants)
colnames(onset_segs) <- paste0("onset.", colnames(onset_segs))
stadt <- bind_cols(stadt, onset_segs)
# stressed onset
stressed_onset_segs <- find_segments(stadt$stressed_onset, onset_s)
colnames(stressed_onset_segs) <- paste0("stressed_onset.", colnames(stressed_onset_segs))
stadt <- bind_cols(stadt, stressed_onset_segs)
# rhyme
rhyme_segs <- find_segments(stadt$rhyme, unique_segments)
colnames(rhyme_segs) <- paste0("rhyme.", colnames(rhyme_segs))
stadt <- bind_cols(stadt, rhyme_segs)
# stressed rhyme
stressed_rhyme_segs <- find_segments(stadt$stressed_rhyme, unique_segments)
colnames(stressed_rhyme_segs) <- paste0("stressed_rhyme.", colnames(stressed_rhyme_segs))
stadt <- bind_cols(stadt, stressed_rhyme_segs)
```

We now add the Proto-Indo-European reconstructions.

```{r}
eng_hist <- read_csv('../raw_data/a1_english_norms/english_historical_info_OED_watkins_2000.csv')
eng_hist <- eng_hist %>% rename(PIE_root="IE_root_watkins_2000")

# merge with norms:

stadt <- left_join(stadt, dplyr::select(eng_hist, Word, PIE_root))
```

Saving clean norm file.

```{r}
write_csv(stadt, '../final_data/english_norms.csv')
```

We now process data from the CMU pronunciation dictionary and SubtLEX for a baseline comparison of the rates of /r/ in adjectives at large vs. within touch adjectives.

Load in CMU data:

```{r, message = FALSE, warning = FALSE}
# CMU data:

CMU <- readLines('../raw_data/a1_english_norms/CMU_dict-0.7b.txt')
phones <- read_csv('../raw_data/a1_english_norms/CMU_phonemes.csv')
lyn <- read_csv('../raw_data/a1_english_norms/lynott_connell_2009_adj_norms.csv')

# SUBTLEX data for POS tags:

SUBTL <- read_csv('../raw_data/a1_english_norms/SUBTLEX_US_with_POS.csv') %>% 
  dplyr::select(Word, FREQcount, Percentage_dom_PoS, Dom_PoS_SUBTLEX) %>% 
  rename(POS = Dom_PoS_SUBTLEX,
         POS_prop = Percentage_dom_PoS)
```

Don't need first 1:126 rows which contain meta-data:

```{r}
CMU <- CMU[-c(1:126)]
```

Process:

```{r}
CMU <- str_split(CMU, pattern = ' +')
```

Get words:

```{r}
words <- sapply(CMU, FUN = function(x) x[1])
```

Make lowercase:

```{r}
words <- str_to_lower(words)
```

Get rid of the final numbers in bracketS:

```{r}
words <- str_replace(words, '\\(.+?\\)', '')
```

Get pronunciations, which need to be collapsed (they are separate entries within the vectors, one vector per list entry):

```{r}
pronunciations <- sapply(CMU, FUN = function(x) str_c(x[-1], collapse = '_'))
```

Make into table by splitting again:

```{r}
pronunciations <- str_split(pronunciations, pattern = '_', simplify = TRUE)
```

Get rid of stress indicators (numbers):

```{r}
pronunciations <- apply(pronunciations, 2, function(x) str_replace(x, '[0-9]', ''))
```

For each word, specify whether it contains the phoneme or not:

```{r}
all_phon <- matrix(rep(0, nrow(pronunciations) * nrow(phones)),
                   nrow = nrow(pronunciations))
colnames(all_phon) <- phones$Phoneme
```

Loop through and fill matrix with counts of phonemes:

```{r}
for (i in 1:nrow(pronunciations)) {
  matches <- match(pronunciations[i, ], phones$Phoneme)
  
  # Get rid of non-matches (NAs):
  
  matches <- matches[!is.na(matches)]
  
  # Get a table for how many matches there are:
  
  match_tab <- table(matches)
  
  # The number of the match needs to be added to the respective column:
  
  all_phon[i, as.numeric(names(match_tab))] <- as.vector(match_tab)
  
  # Tell the external world where you are at:
  
  if (i %% 10000 == 0) cat(str_c(i, '\n'))
}
```

Make a tibble out of this together with the word:

```{r}
CMU_dict <- bind_cols(tibble(Word = words), as_tibble(all_phon))
```

We'll focus on presence/absence, not the precise count:

```{r}
# Transform into 0/1 presence/absence variable:

CMU_dict <- CMU_dict %>% mutate_if(is.double,
                                   function(x) ifelse(x >= 1, 1, 0))
```

Get all adjectives — criterion is 80% of the uses in SUBTLEX need to be adjective:

```{r}
these_adjs <- filter(SUBTL,
                     POS == 'Adjective', POS_prop > 0.8) %>% pull(Word)

CMU_adj <- filter(CMU_dict, Word %in% these_adjs)
```

Remove duplicates.

```{r}
CMU_adj <- filter(CMU_adj, !duplicated(Word))
```

Create indicator variable for sensory adjectives based on whether they are included in Lynott & Connell's (2009) set of sensory adjectives. This will be taken as a baseline of "sensory adjectives".

```{r}
these_lyn <- lyn %>% pull(Word)
CMU_lyn <- CMU_dict %>%
  filter(Word %in% these_lyn) %>%
  filter(!duplicated(Word))
```

Write to CSV.

```{r}
write_csv(CMU_adj, '../final_data/CMU_adj.csv')
write_csv(CMU_lyn, '../final_data/CMU_lyn.csv')
```
