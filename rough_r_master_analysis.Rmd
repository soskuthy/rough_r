---
title: "Rough R - Master analysis file"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---

This is the master analysis file for our paper titled "R is for rough: An iconic universal based on touch". The paper contains a set of different analyses, which are presented in separate sections below. Each section is self-contained: libraries and data files are loaded in the top code chunk, and after running this, all the code for the relevant section should be possible to run.

## 1. English ratings

The data we use here are roughness ratings for 100 English adjectives from Stadtlander & Murdoch (2000). We first present a random forest analysis, followed by a Bayesian linear regression model with roughness ratings as outcome and presence of R as predictors.

We start the random forest analysis with a general test that simply looks at which phonemes are most predictive of roughness ratings. This is followed by a more exploratory analysis that aims to find out which parts of a word contribute the most to this effect. As part of this analysis, we compare several models:

- all onset consonants
- onset consonants in stressed syllables
- all rhyme segments (vowels and consonants)
- rhyme segments (vowels and consonants) in stressed syllables

Loading packages, loading data.

```{r}
library(tidyverse)
library(effsize)
library(ranger)
library(brms)

source("scripts/rough_helper.r")

eng <- read_csv("final_data/english_norms.csv")
eng$rough <- eng$Rough.M >= 0
```

### 1.1 Random forest analysis

Fitting random forest models. Starting with general model and then more specific ones.

```{r}
# 1) general model (with predictors of the form "is segment X present anywhere in the wordform?")
eng_rf_all_preds <- grep("all.", colnames(eng), value=T)
eng_rf_all_formula <- as.formula(paste0('Rough.M ~ ', paste(eng_rf_all_preds, collapse=" + ")))

set.seed(42)
eng_rf_mod_all <- ranger(eng_rf_all_formula,
	 data = eng, mtry = round(sqrt(length(eng_rf_all_preds))), num.trees = 5000,
	 importance = 'permutation')
saveRDS(eng_rf_mod_all, "models/eng_rf_mod_all.rds")

# 2) onset only
eng_rf_onset_preds <- grep("^onset.", colnames(eng), value=T)
eng_rf_onset_formula <- as.formula(paste0('Rough.M ~ ', paste(eng_rf_onset_preds, collapse=" + ")))

set.seed(42)
eng_rf_mod_onset <- ranger(eng_rf_onset_formula,
	 data = eng, mtry = round(sqrt(length(eng_rf_onset_preds))), num.trees = 5000,
	 importance = 'permutation')

# 3) stressed onset only
eng_rf_stressed_onset_preds <- grep("^stressed_onset.", colnames(eng), value=T)
eng_rf_stressed_onset_formula <- as.formula(paste0('Rough.M ~ ', paste(eng_rf_stressed_onset_preds, collapse=" + ")))

set.seed(42)
eng_rf_mod_stressed_onset <- ranger(eng_rf_stressed_onset_formula,
	 data = eng, mtry = round(sqrt(length(eng_rf_stressed_onset_preds))), num.trees = 5000,
	 importance = 'permutation')

# 4) rhyme only
eng_rf_rhyme_preds <- grep("^rhyme.", colnames(eng), value=T)
eng_rf_rhyme_formula <- as.formula(paste0('Rough.M ~ ', paste(eng_rf_rhyme_preds, collapse=" + ")))

set.seed(42)
eng_rf_mod_rhyme <- ranger(eng_rf_rhyme_formula,
	 data = eng, mtry = round(sqrt(length(eng_rf_rhyme_preds))), num.trees = 5000,
	 importance = 'permutation')

# 5) stressed rhyme only
eng_rf_stressed_rhyme_preds <- grep("^stressed_rhyme.", colnames(eng), value=T)
eng_rf_stressed_rhyme_formula <- as.formula(paste0('Rough.M ~ ', paste(eng_rf_stressed_rhyme_preds, collapse=" + ")))

set.seed(42)
eng_rf_mod_stressed_rhyme <- ranger(eng_rf_stressed_rhyme_formula,
	 data = eng, mtry = round(sqrt(length(eng_rf_stressed_rhyme_preds))), num.trees = 5000,
	 importance = 'permutation')
```

Now that we have fit the random forest models, we can look at their prediction accuracy (correlation between predicted vs. actual roughness ratings).

```{r}
sqrt(eng_rf_mod_all$r.squared)            # 0.255
sqrt(eng_rf_mod_onset$r.squared)          # 0.315
sqrt(eng_rf_mod_stressed_onset$r.squared) # 0.361
sqrt(eng_rf_mod_rhyme$r.squared)          # negative r-squared
sqrt(eng_rf_mod_stressed_rhyme$r.squared) # negative r-squared
```

Variance importance for two best-performing models:

```{r}
par(mai = c(1.5, 1, 0.5, 0.5))
plot_varimp(tail(sort(eng_rf_mod_all$variable.importance), 15), xaxis = seq(-0.3, 3, 0.3))
abline(v=abs(min(eng_rf_mod_all$variable.importance)), lty=2)

par(mai = c(1.5, 1, 0.5, 0.5))
plot_varimp(tail(sort(eng_rf_mod_stressed_onset$variable.importance), 15), xaxis = seq(-0.3, 3, 0.3))
abline(v=abs(min(eng_rf_mod_stressed_onset$variable.importance)), lty=2)
```

The graphs for the paper are assembled in a separate file. (TODO)

### 1.2 Descriptive stats

We now calculate some simple descriptive statistics for reporting in the paper and then move on to the statistical modelling. We split the data into "rough" and "smooth" subsets at zero of the rating scale; this is partly justified by the heavily bimodal distribution of the data:

```{r}
eng$rough <- eng$Rough.M >= 0

# density plot
ggplot(eng, aes(x=Rough.M)) +
  geom_density(fill="deepskyblue4")
```

Here are the proportions of /r/ in rough vs. smooth words, along with Cohen's D for the difference in roughness ratings across words with / without /r/.

```{r}
# proportion of /r/ in rough vs. smooth words
eng %>%
  count(rough, all.r) %>%
  group_by(rough) %>%
  summarise(prop.r=sum(all.r * n) / sum(n))

# Cohen's D
cohen.d(Rough.M ~ all.r, eng, pooled=T)

# difference in means:
lm(Rough.M ~ all.r, eng)
```

The same proportions focusing on stressed onsets only.

```{r}
# proportion of stressed onset /r/ in rough vs. smooth words
eng %>%
  count(rough, stressed_onset.r) %>%
  group_by(rough) %>%
  summarise(prop.r=sum(stressed_onset.r * n) / sum(n))
```

### 1.3 Bayesian beta regression with ratings as outcome

We now fit a Bayesian beta regression model with roughness ratings as the outcome and presence of R as the predictor. A beta regression is used since the ratings are bounded on both sides.

```{r, warning=F, message=F}
# in order for beta regression to work, outcome has to be in [0,1]
eng$Rough.M.beta <- (eng$Rough.M + 7)/14

set.seed(314)
eng_beta_mod <- brm(Rough.M.beta ~ all.r,
                   data=eng,
                   family="beta",
                   refresh=0)
summary(eng_beta_mod)

# posterior predictive check: fit not fantastic due to bimodality
pp_check(eng_beta_mod)
```

We now provide a summary of this model based on the posterior distribution.

```{r}
x <- beta_summary(eng_beta_mod, eng, rpred="all.r", binary_pred=F, printPlease=T)
```

### 1.4 Bayesian logistic regression with roughness as outcome

The bimodality of the ratings scale (which is not entirely removed by adding the L and R predictors) means that the results from the model above ought to be treated with caution. For this reason (and for comparability with the PIE model below), we've also run two logistic regression models with presence of L / presence of R as the outcome and binary roughness as the predictor.

```{r, warning=F, message=F}
eng_brm_all.x_priors <- c(set_prior("student_t(5,0,2.5)", class = "b"),
                          set_prior("student_t(5,0,2.5)", class = "Intercept"))
set.seed(314)
eng_brm_all.r_mod <- brm(all.r ~ rough,
                         data=eng,
                         prior=eng_brm_all.x_priors,
                         family="bernoulli",
                         refresh=0)
summary(eng_brm_all.r_mod)

# the same model with a continuous predictor:
#set.seed(314)
#eng_brm_all.r_cont_mod <- brm(all.r ~ Rough.M,
#                         data=eng,
#                         prior=eng_brm_all.x_priors,
#                         family="bernoulli",
#                         refresh=0)
#summary(eng_brm_all.r_cont_mod)
# note: qualitatively the same outcome (with the 95% CrI around Rough.M far from 0)

```

Model predictions.

```{r}
x <- logistic_summary_hard(eng_brm_all.r_mod, eng, outcome="/r/", roughpred="rough", pp_over_zero=T)
```
### 1.5 Baseline comparison for English adjectives

We now estimate the rate of /r/ in English adjectives in general, and sensory adjectives specifically.

```{r}
CMU_adj <- read_csv("final_data/CMU_adj.csv")
CMU_lyn <- read_csv("final_data/CMU_lyn.csv")
```

The rate of /r/ in adjectives (note that this data set contains phoneme rates for all phonemes in US English).

```{r}
#  Sum across them:

adj_sums <- CMU_adj %>% select(-Word) %>% colSums()
adj_props <- adj_sums / nrow(CMU_adj)
adj_props <- round(adj_props, 2)
adj_props["R"]

# How many adjectives in total?

nrow(CMU_adj)
```

The rate of /r/ in sensory adjectives.

```{r}
#  Sum across them:
lyn_sums <- CMU_lyn %>% select(-Word) %>% colSums()
lyn_props <- lyn_sums / nrow(CMU_lyn)
lyn_props <- round(lyn_props, 2)
lyn_props["R"]

# How many adjectives in total?

nrow(CMU_lyn)
```


## 2. IE Google Translate data

The data that we analyse here are Google translations of the words from the Stadtlander & Murdoch (2000) dataset into a variety of languages. For now, we focus on the Indo-European subset of these languages (the non-IE subset will be analysed later). The analysis here is simple: we focus on the binary roughness predictor (as, again, a fine-grained roughness measure does not make sense, given the coarseness of our methods), and look at the proportion of /r/ and /l/ in rough vs. smooth words.

```{r}
library(tidyverse)
library(brms)

source("scripts/rough_helper.r")

trs_IE <- read_csv("final_data/google_translate.csv") %>%
  filter(stock=="Indo-European")
unique(trs_IE$language)
```

Raw descriptive stats & plot.

```{r}
# proportion of /r/ in rough vs. smooth words
trs_IE %>%
  count(rough, r) %>%
  group_by(rough) %>%
  summarise(prop.r=sum(r * n) / sum(n))

# by-language bar plots for /r/
trs_IE %>%
  count(language, rough, r) %>%
  group_by(language, rough) %>%
  mutate(prop=n / sum(n)) %>%
ggplot(aes(x=rough, y=prop, fill=r)) +
  facet_wrap(~language) +
  geom_bar(stat="identity") +
  scale_fill_manual(values=c("deepskyblue4","firebrick3"))
```

The bar plots make it clear that overwhelming majority of languages conform to the rough /r/ pattern. Here's a count of languages where the proportions go in the predicted direction.

```{r}
# more /r/'s in rough words per language
more_rs_in_rough <- trs_IE %>%
  count(language, rough, r) %>%
  group_by(language, rough) %>%
  mutate(prop=n / sum(n)) %>%
  ungroup() %>%
  filter(r) %>%
  complete(language, rough, fill=list(r=T, n=0, prop=0)) %>%
  group_by(language) %>%
  summarise(more_rs_in_rough = (prop[rough] > prop[!rough])) %>%
  ungroup()
cat("The proportion of /r/'s is higher in rough words in", 
    sum(more_rs_in_rough$more_rs_in_rough), "out of",
    nrow(more_rs_in_rough), "languages, which is",
    round(sum(more_rs_in_rough$more_rs_in_rough)*100 / nrow(more_rs_in_rough),2),
    "per cent\n")
```

Bayesian mixed effects logistic modelling (similar to above, but now with additional random effects).

```{r, warning=F, message=F}
trs_IE_brm_priors <- c(
  set_prior("student_t(5,0,2.5)", class = "b"),
  set_prior("student_t(5,0,2.5)", class = "Intercept"),
  set_prior("lkj(2)", class = "cor"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "Intercept", group="mbranch"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "roughTRUE", group="mbranch"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "Intercept", group="language"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "roughTRUE", group="language"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "Intercept", group="eng_orig")
)

set.seed(314)
trs_IE_brm_r_mod <- brm(r ~ rough +
                          (1 + rough | mbranch) +
                          (1 + rough | language) +
                          (1 | eng_orig),
                        data=trs_IE,
                        prior=trs_IE_brm_priors,
                        family="bernoulli",
                        control=list(adapt_delta=0.99),
                        refresh=0)
summary(trs_IE_brm_r_mod)
#saveRDS(trs_IE_brm_r_mod, "models/trs_IE_brm_r_mod.rds")
```

Model predictions.

```{r}
x <- logistic_summary(trs_IE_brm_r_mod, eng, outcome="/r/", roughpred="rough", pp_over_zero=T)
```

We now ask: looking at the exact same words (i.e. same origin, e.g. borrowing, from PIE, etc.), but focusing on other common sounds, do we observe the same degree of stability in the association of roughness with those sounds across languages? Looking at it from the perspective of PIE: do random associations between sounds other than /r/ and roughness remain as consistent across the daughter languages as that between /r/ and roughness? We look at two different things:

- how consistent is the percentage of /r/ in rough vs. smooth words across languages?
- how consistent is the size of the difference in the rate of /r/ between rough vs. smooth words across languages?

```{r}
phons <- c("r", "l", "n", "m", "s", "k", "t", "d", "p", "b")

# variance across languages in rates in rough / smooth words
# note: replacement for proportions of 0 / 1 chosen
# based on the second lowest proportion in the data set, which
# is close to 0.03; same qualitative results are obtained with
# 0.01 and 0.02; if 0/1 proportions are filtered instead of replaced,
# only b has lower variance in rough words (which is an artefact
# of b having a lot of zeros and close to zero values)

logodd <- function (x, replacement=0.03) {
  x_mod <- ifelse(x==0, replacement, x)
  x_mod <- ifelse(x_mod==1, 1-replacement, x_mod)
  return(qlogis(x_mod))
}

pivot_longer(trs_IE,
             phons,
             names_to="phoneme",
             values_to="presence") %>%
  group_by(language, phoneme, rough) %>%
  summarise(phon_prop = mean(presence)) %>%
  ungroup() %>%
  group_by(phoneme, rough) %>%
  summarise(sd = sd(phon_prop),
            sd_logodd = sd(logodd(phon_prop))) %>%
  ungroup() %>%
  filter(rough) %>%
  arrange(sd_logodd)

# variance across languages in strength of effect

pivot_longer(trs_IE,
             phons,
             names_to="phoneme",
             values_to="presence") %>%
  group_by(language, phoneme, rough) %>%
  summarise(phon_prop = mean(presence),
            phon_logodd = logodd(phon_prop, repl=0.03)) %>%
  ungroup() %>%
  group_by(language, phoneme) %>%
  summarise(logodd_diff = phon_logodd[rough] - phon_logodd[!rough]) %>%
  ungroup() %>%
  group_by(phoneme) %>%
  summarise(logodd_diff_sd = sd(logodd_diff)) %>%
  ungroup()  %>%
  arrange(logodd_diff_sd)
```

It is also interesting to note that /r/ is by far the most frequent phoneme (out of the 10 that we look at) in rough words across the languages in our data set, almost 20% more frequent than the second most frequent one (/t/, which is simply a frequent sound cross-linguistically). Conversely, it is only the 4th most frequent phoneme in smooth words.

```{r}

s(time, speaker, bs="fs", m = 1) + 
s(time, speaker, bs="fs", m = 1, by = group.ordered)



# rate of different phonemes in rough / smooth words
pivot_longer(trs_IE,
             phons,
             names_to="phoneme",
             values_to="presence") %>%
  group_by(language, phoneme, rough) %>%
  summarise(phon_prop = mean(presence)) %>%
  ungroup() %>%
  group_by(phoneme, rough) %>%
  summarise(phon_prop = mean(phon_prop)) %>%
  ungroup() %>%
  ggplot(aes(x=phoneme, y=phon_prop, fill=rough)) +
  geom_bar(stat="identity", position=position_dodge())
```

We now check the quality of our translations using Spanish, Polish, Dutch, German and Italian.

```{r}
trs_IE_check <- trs_IE %>%
  filter(!is.na(trans_qual))

# proportion of good / acceptable translations
trs_IE_check %>%
  group_by(language) %>%
  summarise(good_prop = mean(trans_qual %in% c("good", "OK", "ok")))

# mean over languages = 93%
good_trs <- trs_IE_check %>%
  group_by(language) %>%
  summarise(good_prop = mean(trans_qual %in% c("good", "OK", "ok")))
mean(good_trs$good_prop)

# mean over languages for perfect translations only = 77%
good_trs <- trs_IE_check %>%
  group_by(language) %>%
  summarise(good_prop = mean(trans_qual %in% c("good")))
mean(good_trs$good_prop)
```

How do the results for these languages change when we consider only good OR good / acceptable translations?

```{r}
trs_diffs <- trs_IE_check %>%
  group_by(language, rough) %>%
  summarise(r_prop = mean(r),
            r_prop_good_only = mean(r[trans_qual=="good"]),
            r_prop_good_acc  = mean(r[trans_qual %in% c("good", "OK", "ok")])) %>%
  ungroup() %>%
  mutate(good_diff = r_prop - r_prop_good_only,
         good_acc_diff = r_prop - r_prop_good_acc)
sd(trs_diffs$good_diff)
sd(trs_diffs$good_acc_diff)

trs_diffs %>%
  group_by(rough) %>%
  summarise(r_prop = mean(r_prop),
            r_prop_good_only = mean(r_prop_good_only),
            r_prop_good_acc = mean(r_prop_good_acc))
```

## 3. Hungarian ratings

We now move on to our analysis of roughness ratings from Hungarian. These data come from an experiment conducted as part of the current study, where participants rated 85 Hungarian surface descriptors. Some of these adjectives are translations of the Stadtlander & Murdoch (2000) adjectives, while some others are independent of that experiment. For comparability with the English analysis, we use the aggregated data in our analyses, even though a deaggregated data set is available. The deaggregated data could be analysed via zero-inflated beta regression -- but for simplicity, we avoid this here. We first present random forest analyses, which is followed by Bayesian beta regression analyses. The structure of this section is closely analogous to that of section 1.

To make sure that the results here are truly complementary to the Indo-European analysis, we restrict the Hungarian data to those forms that are not of Indo-European origin (i.e. we exclude IE loanwords).

```{r, warning=F, message=F}
library(tidyverse)
library(ranger)
library(brms)

hun_aggr <- read_csv("final_data/hun_norms_aggr.csv") %>%
  filter(!(etymology.source %in% c('slavic', 'germanic', 'greek', 'romance')))

hun_aggr$rough <- hun_aggr$roughness >= 0
```

### 3.1 Random forest analysis

Fitting random forest models to the aggregated data. Starting with general model and then more specific ones.

```{r}
# 1) general model (with predictors of the form "is segment X present anywhere in the wordform?")
# (note: I've tried consonantal models too, but their R (for correlation) is lower at 0.265)
# hun_rf_cons_preds <- grep("all[.][iIoOaAyYøØeE]", grep("all.", colnames(hun_aggr), value=T), value=T, invert=T)
# hun_rf_cons_formula <- as.formula(paste0('roughness ~ ', paste(hun_rf_cons_preds, collapse=" + ")))
hun_rf_all_preds <- grep("all.", colnames(hun_aggr), value=T)
hun_rf_all_formula <- as.formula(paste0('roughness ~ ', paste(hun_rf_all_preds, collapse=" + ")))

set.seed(42)
hun_rf_mod_all <- ranger(hun_rf_all_formula,
	 data = hun_aggr, mtry = round(sqrt(length(hun_rf_all_preds))), num.trees = 5000,
	 importance = 'permutation')
saveRDS(hun_rf_mod_all, "models/hun_rf_mod_all.rds")

# 2) onset only
hun_rf_onset_preds <- grep("^onset.", colnames(hun_aggr), value=T)
hun_rf_onset_formula <- as.formula(paste0('roughness ~ ', paste(hun_rf_onset_preds, collapse=" + ")))

set.seed(42)
hun_rf_mod_onset <- ranger(hun_rf_onset_formula,
	 data = hun_aggr, mtry = round(sqrt(length(hun_rf_onset_preds))), num.trees = 5000,
	 importance = 'permutation')

# 3) stressed onset only
hun_rf_stressed_onset_preds <- grep("^stressed_onset.", colnames(hun_aggr), value=T)
hun_rf_stressed_onset_formula <- as.formula(paste0('roughness ~ ', paste(hun_rf_stressed_onset_preds, collapse=" + ")))

set.seed(42)
hun_rf_mod_stressed_onset <- ranger(hun_rf_stressed_onset_formula,
	 data = hun_aggr, mtry = round(sqrt(length(hun_rf_stressed_onset_preds))), num.trees = 5000,
	 importance = 'permutation')

# 4) rhyme only
hun_rf_rhyme_preds <- grep("^rhyme.", colnames(hun_aggr), value=T)
hun_rf_rhyme_formula <- as.formula(paste0('roughness ~ ', paste(hun_rf_rhyme_preds, collapse=" + ")))

set.seed(42)
hun_rf_mod_rhyme <- ranger(hun_rf_rhyme_formula,
	 data = hun_aggr, mtry = round(sqrt(length(hun_rf_rhyme_preds))), num.trees = 5000,
	 importance = 'permutation')

# 5) stressed rhyme only
hun_rf_stressed_rhyme_preds <- grep("^stressed_rhyme.", colnames(hun_aggr), value=T)
hun_rf_stressed_rhyme_formula <- as.formula(paste0('roughness ~ ', paste(hun_rf_stressed_rhyme_preds, collapse=" + ")))

set.seed(42)
hun_rf_mod_stressed_rhyme <- ranger(hun_rf_stressed_rhyme_formula,
	 data = hun_aggr, mtry = round(sqrt(length(hun_rf_stressed_rhyme_preds))), num.trees = 5000,
	 importance = 'permutation')
```

Now that we have fit the random forest models, we can look at their prediction accuracy (correlation between predicted vs. actual roughness ratings).

```{r}
sqrt(hun_rf_mod_all$r.squared)            # 0.372
sqrt(hun_rf_mod_onset$r.squared)          # 0.342
sqrt(hun_rf_mod_stressed_onset$r.squared) # negative r-squared
sqrt(hun_rf_mod_rhyme$r.squared)          # 0.178
sqrt(hun_rf_mod_stressed_rhyme$r.squared) # negative r-squared
```

Variance importance for two models (note that /r/ is not the winner for either model, though it's pretty much tied for first place for the first one).

```{r}
par(mai = c(1.5, 1, 0.5, 0.5))
plot_varimp(tail(sort(hun_rf_mod_all$variable.importance), 15), xaxis = seq(-0.3, 3, 0.3))
abline(v=abs(min(hun_rf_mod_all$variable.importance)), lty=2)

par(mai = c(1.5, 1, 0.5, 0.5))
plot_varimp(tail(sort(hun_rf_mod_onset$variable.importance), 15), xaxis = seq(-0.3, 3, 0.3))
abline(v=abs(min(eng_rf_mod_stressed_onset$variable.importance)), lty=2)
```

A side note: while /r/ does not seem to have the highest variable importance in the models, it is (1) among the most important variables and (2) it is the most general predictor out of all the ones listed here insofar as almost half of all of our forms contain an /r/.

```{r}
mean(hun_aggr$all.r) # sum(hun_aggr$all.r): that's 29 items
mean(hun_aggr$all.ø) # sum(hun_aggr$all.ø): that's literally *3* items!
mean(hun_aggr$all.m)
mean(hun_aggr$all.d)
mean(hun_aggr$all.H)
```

### 3.2 Descriptive stats

We now calculate some simple descriptive statistics for reporting in the paper and then move on to the statistical modelling. These ratings are not nearly as bimodal as the English ones, but there is still some bimodality and it is useful to employ a binary split for comparability with the English data.

```{r}
# density plot
ggplot(hun_aggr, aes(x=roughness)) +
  geom_density(fill="deepskyblue4")
```

Here are the proportions of /r/ in rough vs. smooth words, along with Cohen's D for the difference in roughness ratings across words with / without /r/.

```{r}
# proportion of /r/ in rough vs. smooth words
hun_aggr %>%
  count(rough, all.r) %>%
  group_by(rough) %>%
  summarise(prop.r=sum(all.r * n) / sum(n))

# Cohen's D
cohen.d(roughness ~ all.r, hun_aggr, pooled=T)

# difference in means:
lm(roughness ~ all.r, hun_aggr)

```

There's not much point in calculating these proportions for onset / stressed onset / other positions, as the best model was the one fit to the whole word.

### 3.3 Bayesian beta regression with ratings as outcome

We now fit a Bayesian beta regression model with roughness ratings as the outcome and presence of R as the predictors. A beta regression is used since the ratings are bounded on both sides.

```{r, warning=F, message=F}
# in order for beta regression to work, outcome has to be in [0,1]
hun_aggr$roughness.beta <- (hun_aggr$roughness + 7)/14

set.seed(314)
hun_brm_beta_mod <- brm(roughness.beta ~ all.r,
                          data=hun_aggr,
                          family="beta",
                          refresh=0)
summary(hun_brm_beta_mod)

# posterior predictive check: fit okish, could be better
pp_check(hun_brm_beta_mod)
```

We now provide a summary of this model based on the posterior distribution.

```{r}
# binary pred set to false because all.r / all.l are coded as numeric 0 vs. 1
preds <- beta_summary(hun_brm_beta_mod, hun_aggr, rpred="all.r", binary_pred=F, printPlease=T)
```

### 3.4 Bayesian logistic regression with roughness as outcome

A regression model with presence of R as the outcome and binary roughness as the predictor. We can only run this model on the aggregated data: the proportion of /r/ is fixed within subjects (as they all rated the same stimuli) and also within stimuli, so we wouldn't be able to include those as fixed effects; but there are dependencies within subjects and items!

```{r, warning=F, message=F}
hun_brm_priors <- c(
  set_prior("student_t(5,0,2.5)", class = "b"),
  set_prior("student_t(5,0,2.5)", class = "Intercept"))

set.seed(314)
hun_brm_r_mod <- brm(all.r ~ rough,
                        data=hun_aggr,
                        prior=hun_brm_priors,
                        family="bernoulli",
                        refresh=0)
summary(hun_brm_r_mod)
```

Model predictions.

```{r}
x <- logistic_summary(hun_brm_r_mod, eng, outcome="/r/", roughpred="rough", pp_over_zero=T)
```

We note that running these models on the full data set (i.e. not restricted to words of non-Indo-European origin) yields exactly the same results.

```{r, warning=F, message=F}
hun_ie <- read_csv("final_data/hun_norms_aggr.csv")
hun_ie$rough <- hun_ie$roughness >= 0

set.seed(314)
hun_ie_brm_r_mod <- brm(all.r ~ rough,
                        data=hun_ie,
                        prior=hun_brm_priors,
                        family="bernoulli",
                        refresh=0)
summary(hun_ie_brm_r_mod)
```

Model predictions.

```{r}
x <- logistic_summary(hun_ie_brm_r_mod, eng, outcome="/r/", roughpred="rough", pp_over_zero=T)
```


## 4. Cross-linguistic analysis

The latest version of our cross-linguistic data set includes 396 languages from 85 language families (counting isolates as families) from 19 Autotyp areas. Without Indo-European, there are 350 languages and 84 families from 19 Autotyp areas. We exclude Indo-European as we've already analysed these languages above.

```{r}
library(tidyverse)
library(brms)

source("scripts/rough_helper.r")

xling <- read_csv("final_data/cross_linguistic.csv")
length(unique(xling$Language))
length(unique(xling$Family))
length(unique(xling$Area))
xling <- filter(xling, Family!="Indo-European")
length(unique(xling$Language))
length(unique(xling$Family))
length(unique(xling$Area))
```

Our main analysis focuses specifically on the words rough and smooth. We report numbers for these in the paper. 681 words, 332 languages, 84 families, 19 areas.

```{r}
xling_rs <- filter(xling, Meaning %in% c("rough","smooth"))
nrow(xling_rs)
length(unique(xling_rs$Language))
length(unique(xling_rs$Family))
length(unique(xling_rs$Area))
```

Of these, trilled only: 387 words, 179 languages, 43 families, 18 areas.

```{r}
# Trilled
xling_trill_rs <- filter(xling_rs, Trill=="yes")
nrow(xling_trill_rs)
length(unique(xling_trill_rs$Language))
length(unique(xling_trill_rs$Family))
length(unique(xling_trill_rs$Area))
```

And non-trilled only: 294 words, 153 languages, 61 families, 16 areas.

```{r}
# Non-trilled
xling_no_trill_rs <- filter(xling_rs, Trill=="no")
nrow(xling_no_trill_rs)
length(unique(xling_no_trill_rs$Language))
length(unique(xling_no_trill_rs$Family))
length(unique(xling_no_trill_rs$Area))

```

Due to our extraction methods, only data points from the Google Translate data set and Chirila may have meanings other than "rough" (53 out of 188 languages) or "smooth" (54 out of 310 languages). Sometimes, a single language may have multiple words representing "rough"/"smooth". Overall, it is rare for a language to be represented in our data by words with meanings other "rough" / "smooth". The data set is fairly balanced across rough vs. smooth words.

```{r}
xling %>%
  dplyr::count(Language, rough) %>%
  complete(Language, rough, fill=list(n=0)) %>%
  ggplot(aes(x=n)) + 
  facet_grid(rough ~.) +
  geom_histogram(binwidth=1)

# how many rough words that have a meaning other than "rough";
# how many smooth words that have a meaning other than "smooth";

xling %>%
  group_by(Language) %>%
  summarise(rough_not_rough = sum(rough & Meaning != "rough"),
            smooth_not_smooth = sum(!rough & Meaning != "smooth")) %>%
  ungroup() %>%
  summarise(rough_not_rough_n = sum(rough_not_rough > 0),
            smooth_not_smooth_n = sum(smooth_not_smooth > 0))

# overall Ns

xling %>%
  group_by(Language) %>%
  summarise(rough = sum(Meaning == "rough"),
            smooth = sum(Meaning == "smooth")
  ) %>%
  ungroup() %>%
  summarise(rough_n = sum(rough > 0),
            smooth_n = sum(smooth > 0))

xling %>%
  group_by(Dataset, Language) %>%
  summarise(rough_not_rough = sum(rough & Meaning != "rough"),
            smooth_not_smooth = sum(!rough & Meaning != "smooth")) %>%
  ungroup() %>%
  filter(rough_not_rough > 0 | smooth_not_smooth > 0) %>%
  pull(Dataset)
```

Our analysis will distinguish between languages with trilled /r/s vs. languages that have /r/s that are not trilled. There are quite a few languages where this info is either not available, or where the language has both trills and other rhotics; these languages are excluded from the analysis below.

A previous version of this file (still available via earlier GitHub commits) presented analyses for words with any meaning in the data followed by analyses for words specifically with the meanings "rough" / "smooth". Given the imbalances shown above (few languages where we have data for words other than "rough" / "smooth"), here we focus on "rough" / "smooth" only to avoid further increasing the length of this analysis file. Note that the results of those previous analyses are all in line with our findings for "rough" / "smooth" only, but with a slightly attenuated effect size. The summaries of the raw data below show this clearly (first summary: all words; second summary: "rough"/"smooth" only):

```{r}
# proportions of /r/ in rough / smooth meanings for languages with trilled
# vs. languages without trilled r in words with any meaning
xling %>% 
  group_by(Language, Trill, rough) %>%
  summarise(r_prop = mean(r)) %>%
  ungroup() %>%
  group_by(Trill, rough) %>%
  summarise(r_prop = mean(r_prop)) %>%
  ungroup()

# proportions of /r/ in the words "rough" / "smooth" for languages with
# trilled vs. languages without trilled r
xling %>% 
  filter(Meaning %in% c("rough", "smooth")) %>%
  group_by(Language, Trill, rough) %>%
  summarise(r_prop = mean(r)) %>%
  ungroup() %>%
  group_by(Trill, rough) %>%
  summarise(r_prop = mean(r_prop)) %>%
  ungroup()
```

Here is the plan for our cross-linguistic analysis:
- We'll analyse languages with trills vs. those with no trills separately (as we only have a prediction for Ls with trills).
- We use logistic models with r as outcome, binary rough as predictor; random effects by Family, Area
 
### 4.1 Cross-linguistic analysis of languages with trills

Limiting the data set to languages with trills only and the words "rough" / "smooth" leaves 387 data points from 179 languages representing 43 families and 18 areas.

```{r}
xling_trill_rs <- filter(xling, Trill=="yes", Meaning %in% c("rough","smooth"))
nrow(xling_trill_rs)
length(unique(xling_trill_rs$Language))
length(unique(xling_trill_rs$Family))
length(unique(xling_trill_rs$Area))
```

We model the presence of r as a function of roughness (binary) in the cross-linguistic data set limited to the words rough/smooth only (the expectation is that the effect would be particularly strong here). We include random effects by Family/Area (not by language, as most languages only have one rough and one smooth word).

```{r, warning=F, message=F}
# priors: since the outcomes here are similar to those for a logistic model (i.e. a number between 0/1),
# and the link function is the same, we use the same priors as we do for logistic models

xling_brm_rs_logistic_priors <- c(
  set_prior("student_t(5,0,2.5)", class = "b"),
  set_prior("student_t(5,0,2.5)", class = "Intercept"),
  set_prior("lkj(2)", class = "cor"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "Intercept", group="Family"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "roughTRUE", group="Family"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "Intercept", group="Area"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "roughTRUE", group="Area")
)

set.seed(314)
xling_brm_rs_logistic_mod_r <- brm(r ~ rough +
                          (1 + rough | Family) +
                          (1 + rough | Area),
                        data=xling_trill_rs,
                        prior=xling_brm_rs_logistic_priors,
                        family="bernoulli",
                       control=list(adapt_delta=0.9),
                       refresh=0)
#summary(xling_brm_rs_logistic_mod_r)
saveRDS(xling_brm_rs_logistic_mod_r, "models/xling_brm_rs_logistic_mod_r.rds")
```

Model predictions.

```{r}
x <- logistic_summary(xling_brm_rs_logistic_mod_r, dat=xling_trill_rs, outcome="/r/", roughpred="rough", pp_over_zero=T)
```

### 4.2 Cross-linguistic analysis of languages *without* trills

Limiting the data set to languages without trills (or where it's not clear whether the language has a trill) and the words "rough"/"smooth" leaves 294 data points from 153 languages representing 61 families and 16 areas.

```{r}
xling_no_trill_rs <- filter(xling_no_trill, Trill=="no", Meaning %in% c("rough","smooth"))
nrow(xling_no_trill_rs)
length(unique(xling_no_trill_rs$Language))
length(unique(xling_no_trill_rs$Family))
length(unique(xling_no_trill_rs$Area))
```

We model the presence of r as a function of roughness (binary) in the cross-linguistic data set limited to the words rough/smooth only. We include random effects by Family/Area as above.

```{r, warning=F, message=F}
# priors: since the outcomes here are similar to those for a logistic model (i.e. a number between 0/1),
# and the link function is the same, we use the same priors as we do for logistic models

xling_brm_rs_logistic_priors <- c(
  set_prior("student_t(5,0,2.5)", class = "b"),
  set_prior("student_t(5,0,2.5)", class = "Intercept"),
  set_prior("lkj(2)", class = "cor"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "Intercept", group="Family"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "roughTRUE", group="Family"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "Intercept", group="Area"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "roughTRUE", group="Area")
)

set.seed(314)
xling_nt_brm_rs_logistic_mod_r <- brm(r ~ rough +
                          (1 + rough | Family) +
                          (1 + rough | Area),
                        data=xling_no_trill_rs,
                        prior=xling_brm_rs_logistic_priors,
                        family="bernoulli",
                       control=list(adapt_delta=0.9),
                       refresh=0)
saveRDS(xling_nt_brm_rs_logistic_mod_r, "models/xling_nt_brm_rs_logistic_mod_r.rds")
```

Model predictions as usual. No patterns!

```{r}
x <- logistic_summary(xling_nt_brm_rs_logistic_mod_r, dat=xling_no_trill_rs, outcome="/r/", roughpred="rough", pp_over_zero=T)
```

### 4.3 Omnibus analysis: languages with trills & no-trills together

In this model, we consider all the data simultaneously -- so all languages are included. We add Trill as a predictor to our model (as well as an interaction between Trill and rough).

Data summary:

```{r}
xling_rs <- filter(xling, Meaning %in% c("rough","smooth"),
                   !is.na(Trill))
nrow(xling_rs)
length(unique(xling_rs$Language))
length(unique(xling_rs$Family))
length(unique(xling_rs$Area))
```

It bears mentioning that Trill is very asymmetrical across language families: close to 75% of language families have only trills or no trills, and even those families where both are represented have fairly skewed distributions, with very few where the minority group is represented by more than 1-2 languages. In other words, Trill is almost a between-Family (as opposed to within-Family) predictor. 

```{r}
table(xling_rs$Family, xling_rs$Trill)
xling_rs %>%
  group_by(Family) %>%
  summarise(prop_trill=mean(Trill=="yes")) %>%
  ungroup() %>%
  (function (x) hist(x$prop_trill, 15))
```

In this sense, a model where Trill is included as a random slope by Family may be very difficult to estimate. We'll try anyways, but the uncertainty of the by-family estimates may unduly inflate credible intervals.

```{r}
xling_brm_omnibus_priors <- c(
  set_prior("student_t(5,0,2.5)", class = "b"),
  set_prior("student_t(5,0,2.5)", class = "Intercept"),
  set_prior("lkj(2)", class = "cor"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "Intercept", group="Family"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "roughTRUE", group="Family"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "Trillyes", group="Family"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "roughTRUE:Trillyes", group="Family"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "Intercept", group="Area"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "roughTRUE", group="Area"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "Trillyes", group="Area"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "roughTRUE:Trillyes", group="Area")
)

set.seed(314)
xling_brm_omnibus_mod_r <- brm(r ~ rough * Trill +
                          (1 + rough * Trill | Family) +
                          (1 + rough * Trill | Area),
                        data=xling_rs,
                        prior=xling_brm_omnibus_priors,
                        family="bernoulli",
                       control=list(adapt_delta=0.99))
summary(xling_brm_omnibus_mod_r)
```

The interaction here seems pretty reliable.

```{r}
mean(posterior_samples(xling_brm_omnibus_mod_r)[,"b_roughTRUE:Trillyes"] > 0)

l <- logistic_intr_summary(xling_brm_omnibus_mod_r, xling_rs, outcome="/r/", roughpred="rough",
                      trillpred="Trill", pp_over_zero=T,
                      binary_pred=T, printPlease=T)
```

When the same model is fitted without by-Family random slopes over Trill, essentially the same results are obtained, but with narrower credible intervals.

```{r}
xling_brm_omnibus_no_famtrill_priors <- c(
  set_prior("student_t(5,0,2.5)", class = "b"),
  set_prior("student_t(5,0,2.5)", class = "Intercept"),
  set_prior("lkj(2)", class = "cor"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "Intercept", group="Family"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "roughTRUE", group="Family"),
  #set_prior("student_t(4,0,2)", class = "sd", coef = "Trillyes", group="Family"),
  #set_prior("student_t(4,0,2)", class = "sd", coef = "roughTRUE:Trillyes", group="Family"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "Intercept", group="Area"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "roughTRUE", group="Area"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "Trillyes", group="Area"),
  set_prior("student_t(4,0,2)", class = "sd", coef = "roughTRUE:Trillyes", group="Area")
)

set.seed(314)
xling_brm_omnibus_rs_famtrill_mod_r <- brm(r ~ rough * Trill +
                          (1 + rough | Family) +
                          (1 + rough * Trill | Area),
                        data=xling_rs,
                        prior=xling_brm_omnibus_no_famtrill_priors,
                        family="bernoulli",
                       control=list(adapt_delta=0.99))
summary(xling_brm_omnibus_no_famtrill_mod_r)
```

Note the narrower intervals below.

```{r}
mean(posterior_samples(xling_brm_omnibus_no_famtrill_mod_r)[,"b_roughTRUE:Trillyes"] > 0)

l <- logistic_intr_summary(xling_brm_omnibus_no_famtrill_mod_r, xling_rs, outcome="/r/", roughpred="rough",
                      trillpred="Trill", pp_over_zero=T,
                      binary_pred=T, printPlease=T)
```

# 5. Analysis of roots in Proto-Indo-European (based on Rolf Noyer's data set)

This is essentially a re-run of the analyses above, but now using data sampled directly from PIE. These roots are taken from Rolf Noyer's PIE root database, which was filtered to keep only words with meanings that match those in the English roughness norms data set. Again, we focus on the following question: is the proportion of /r/ higher in rough words than in smooth words?

Loading packages, loading data.

```{r}
library(tidyverse)
library(brms)

source("scripts/rough_helper.r")

pie2 <- read_csv("final_data/pie2.csv")
```

## 5.1 Descriptive stats for roots in PIE based on Rolf Noyer's data set

Here are the proportions of /r/ by rough vs. smooth.

```{r}
pie2 %>%
  count(rough, r) %>%
  group_by(rough) %>%
  summarise(prop.r=sum(r * n) / sum(n))
```

## 5.2 Bayesian modelling for roots in PIE (Rolf Noyer's data)

Same model structure as in e.g. section 1.4.

```{r}
pie2_brm_priors <- c(set_prior("student_t(5,0,2.5)", class = "b"),
                    set_prior("student_t(5,0,2.5)", class = "Intercept"))
set.seed(314)
pie2_brm_mod <- brm(r ~ rough,
                   data=pie2,
                   prior=pie2_brm_priors,
                   family="bernoulli",
                   refresh=0)
summary(pie2_brm_mod)
```

Model predictions.

```{r}
x <- logistic_summary(pie2_brm_mod, pie, outcome="/r/", roughpred="rough", pp_over_zero=T)
```

## 6. Other perceptual dimensions

We will now see if /r/ correlates with other perceptual dimensions.

Loading data.

```{r}
library(tidyverse)
library(effsize)
library(ranger)
library(brms)

source("scripts/rough_helper.r")


stadt <- read_csv("final_data/stadt_all_norms.csv")
stadt <- rename(stadt, all.schwa="all.@")
stadt$rough <- stadt$rough_M >= 0

eng <- read_csv("final_data/english_norms.csv")
stadt <- filter(stadt, (!is.na(rough_M) & (Word %in% eng$Word)) | is.na(rough_M) )
```

Cohen's d and raw effect size for all perceptual dimensions.

```{r}
# Cohen's D
cohen.d(rough_M ~ all.r, stadt, pooled=T)
cohen.d(hard_M ~ all.r, stadt, pooled=T)
cohen.d(motion_M ~ all.r, stadt, pooled=T)
cohen.d(temp_M ~ all.r, stadt, pooled=T)
cohen.d(weight_M ~ all.r, stadt, pooled=T)
cohen.d(size_M ~ all.r, stadt, pooled=T)
cohen.d(shape_M ~ all.r, stadt, pooled=T)

# difference in means:
coef(lm(rough_M ~ all.r, stadt))[2]
coef(lm(hard_M ~ all.r, stadt))[2]
coef(lm(motion_M ~ all.r, stadt))[2]
coef(lm(temp_M ~ all.r, stadt))[2]
coef(lm(weight_M ~ all.r, stadt))[2]
coef(lm(size_M ~ all.r, stadt))[2]
coef(lm(shape_M ~ all.r, stadt))[2]
```

Now looking at the correlation between rough and hard + a Bayesian logistic model with both of them as predictors of /r/.

```{r}
# correlation between rough and hard
cor(stadt$rough_M, stadt$hard_M, use="complete.obs")

# regression
rough_hard_priors <- c(set_prior("student_t(5,0,2.5)", class = "b"),
                       set_prior("student_t(5,0,2.5)", class = "Intercept"))

set.seed(314)
rough_hard_brm <- brm(all.r ~ rough_M + hard_M, 
                      data=stadt,
                      prior=rough_hard_priors,
                      family="bernoulli")
summary(rough_hard_brm)
```